{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import make_scorer\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "import itertools\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import coverage_error\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import label_ranking_average_precision_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import scipy.sparse\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from scipy.sparse import coo_matrix, hstack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49399, 10)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dataset loading\n",
    "dataset = pd.read_csv('Cleaned_Posts.csv', nrows = None, index_col=0)\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TText             0\n",
       "TText_NEG         0\n",
       "PText             0\n",
       "TCode         10054\n",
       "PCode          9627\n",
       "TTitle            0\n",
       "TTitle_NEG        0\n",
       "Title             0\n",
       "PTags             0\n",
       "Tags              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Any np.nan ?\n",
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.nan cleaning\n",
    "dataset['TCode'] = dataset['TCode'].fillna('None')\n",
    "dataset['PCode'] = dataset['PCode'].fillna('None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TText         0\n",
       "TText_NEG     0\n",
       "PText         0\n",
       "TCode         0\n",
       "PCode         0\n",
       "TTitle        0\n",
       "TTitle_NEG    0\n",
       "Title         0\n",
       "PTags         0\n",
       "Tags          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Any np.nan ?\n",
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 10)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Downsampling\n",
    "dataset = dataset.sample(5000)\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tags cleaning\n",
    "dataset['PTags'] = dataset['Tags'].apply(lambda x: [tag.name for tag in BeautifulSoup(x, 'html.parser').find_all()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomLDA(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self,text_n_components = 10):\n",
    "        self.text_n_components = text_n_components\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        dftext = X['TText']\n",
    "        dftitle = X['TTitle']\n",
    "        dfcode = X['TCode']\n",
    "        #Text preparation\n",
    "        self.textcvect = CountVectorizer(tokenizer=None, vocabulary=None)\n",
    "        temp = self.textcvect.fit_transform(dftext)\n",
    "        self.textlda = LatentDirichletAllocation(n_components=50, learning_method = 'batch')\n",
    "        self.textlda.fit(temp)\n",
    "        \n",
    "        #Title preparation\n",
    "        self.titlecvect = CountVectorizer(tokenizer=None, vocabulary=None, )\n",
    "        temp = self.titlecvect.fit_transform(dftitle)\n",
    "        self.titlelda = LatentDirichletAllocation(n_components=50, learning_method = 'batch')\n",
    "        self.titlelda.fit(temp)\n",
    "        \n",
    "        #Code preparation\n",
    "        self.codecvect = CountVectorizer(tokenizer=None, vocabulary=None, )\n",
    "        temp = self.codecvect.fit_transform(dfcode)\n",
    "        self.codelda = LatentDirichletAllocation(n_components=50, learning_method = 'batch')\n",
    "        self.codelda.fit(temp)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        dftext = X['TText']\n",
    "        dftitle = X['TTitle']\n",
    "        dfcode = X['TCode']\n",
    "        \n",
    "        #Text preparation\n",
    "        textvect = self.textcvect.transform(dftext)\n",
    "        textlda = self.textlda.transform(textvect)\n",
    "        \n",
    "        #Title preparation\n",
    "        titlevect = self.titlecvect.transform(dftitle)\n",
    "        titlelda = self.titlelda.transform(titlevect)\n",
    "    \n",
    "        #Code preparation\n",
    "        codevect = self.codecvect.transform(dfcode)\n",
    "        codelda = self.codelda.transform(codevect)\n",
    "        \n",
    "        return np.hstack((textlda, titlelda, codelda))\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        textfnames = self.textcvect.get_feature_names()\n",
    "        titlefnames = self.titlecvect.get_feature_names()\n",
    "        codefnames = self.codecvect.get_feature_names()\n",
    "        toreturn = []\n",
    "        toreturn.extend(textfnames)\n",
    "        toreturn.extend(titlefnames)\n",
    "        toreturn.extend(codefnames)\n",
    "        \n",
    "        return toreturn\n",
    "    \n",
    "    def components_(self):\n",
    "        text = self.textlda.components_\n",
    "        title = self.titlelda.components_\n",
    "        code = self.codelda.components_\n",
    "        return np.hstack((text, title, code))\n",
    "\n",
    "\n",
    "#customLDA = CustomLDA()\n",
    "#customLDA.fit(dataset[['TText', 'TTitle', 'TCode']])\n",
    "#print('Step1 done')\n",
    "#customLDA.transform(textset[['TText', 'TTitle', 'TCode']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomLDA(text_n_components=10)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article = [48098044,]\n",
    "samplepost = dataset[['TText', 'TTitle', 'TCode']].loc[article]\n",
    "\n",
    "\n",
    "customLDA = CustomLDA()\n",
    "customLDA.fit(dataset[['TText', 'TTitle', 'TCode']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = customLDA.transform(samplepost)\n",
    "H = customLDA.components_()\n",
    "feature_names = customLDA.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "public get set packages use\n",
      "Topic 1:\n",
      "name address new value require\n",
      "Topic 2:\n",
      "file use self code the\n",
      "Topic 3:\n",
      "android id key codes string\n",
      "Topic 4:\n",
      "self none java org apache\n",
      "Topic 5:\n",
      "value type use button input\n",
      "Topic 6:\n",
      "if as constraintset value new\n",
      "Topic 7:\n",
      "error this use get return\n",
      "Topic 8:\n",
      "java at org com file\n",
      "Topic 9:\n",
      "div class form id view\n"
     ]
    }
   ],
   "source": [
    "for topic_idx, topic in enumerate(H):\n",
    "    print(\"Topic %d:\" % (topic_idx))\n",
    "    print(\" \".join([feature_names[i] for i in topic.argsort()[:-6:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"set vm argument eclips like valu inject prop file load prop file use spring code config_data_path set success system properti inject prop file prop.getproperti `` test '' print path want simpli print config_data_path\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TText = samplepost['TText'].iloc[0]\n",
    "TText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "customLDA = CustomLDA()\n",
    "X = customLDA.fit_transform(dataset[['TText', 'TTitle', 'TCode']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=500, random_state=0).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tag</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>python</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>android</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>javascript</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>email</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>if-statement</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>physics</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gmail</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>automation</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mongodb</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mysql</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>capacity-planning</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfs-process-template</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfs2015</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amazon-ec2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>momentjs</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>python-3.x</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>symfony</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>routes</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base64</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>python-2.7</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>csv</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pandas</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indexing</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>numpy</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resnet</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conv-neural-network</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keras</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neural-network</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tensorflow</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amazon-elb</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Count\n",
       "Tag                        \n",
       "python                    3\n",
       "android                   2\n",
       "javascript                2\n",
       "email                     2\n",
       "if-statement              1\n",
       "r                         1\n",
       "physics                   1\n",
       "gmail                     1\n",
       "automation                1\n",
       "mongodb                   1\n",
       "mysql                     1\n",
       "capacity-planning         1\n",
       "tfs-process-template      1\n",
       "tfs2015                   1\n",
       "amazon-ec2                1\n",
       "momentjs                  1\n",
       "python-3.x                1\n",
       "symfony                   1\n",
       "routes                    1\n",
       "base64                    1\n",
       "python-2.7                1\n",
       "csv                       1\n",
       "pandas                    1\n",
       "indexing                  1\n",
       "numpy                     1\n",
       "resnet                    1\n",
       "conv-neural-network       1\n",
       "keras                     1\n",
       "neural-network            1\n",
       "tensorflow                1\n",
       "amazon-elb                1"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = kmeans.labels_ == 45\n",
    "popularitydict = {}\n",
    "for tags in list(dataset['PTags'][mask]):\n",
    "    for tag in tags:\n",
    "        if tag in popularitydict.keys():\n",
    "            popularitydict[tag] += 1\n",
    "        else:\n",
    "            popularitydict[tag] = 1\n",
    "df = pd.DataFrame(list(popularitydict.items()))\n",
    "df.columns = ('Tag', 'Count')\n",
    "df.set_index('Tag', inplace = True)\n",
    "df.sort_values('Count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twilio - Using taskrouter.js and reservation.conference() how to not beep and end conference\n",
      "\n",
      " I am using task router to assign an incoming call task to a worker. When the worker gets the reservation I am starting a conference like this: There is not much documentation for how to handle a conference with taskrouter.js, but this seems to work to start the conference. There are 2 problems I am having: I can't stop the 'entering conference' beep to not play When both the worker and participant exit the conference the conference is not actually ended and therefore not putting the worker into the after work activity state. Any help would be appreciated.\n",
      "\n",
      "\n",
      "\n",
      "How to set multiple scroll views size to change dynamically relative to each other?\n",
      "\n",
      " I have two scroll views in a vertical linear layout.\n",
      "I want them to be relative to each other so that they fill the entire linear layout and compensate if one cant cover half the screen. Lets call that scroll views TOP and BOT.\n",
      "If the screen can display 4 rows and both scroll views have infinite rows, each scroll views should display 2 rows and be able to scroll down to se the rest rows. If TOP has 1 and BOT infinite rows, BOT should be resized to 3/4 of the linear layout. If TOP have infinite and Bot has 1 row TOP should still just display 2, i.e. it should never pass the linear layouts vertical center. Here are some pictures for reference: my setup with weight set to 0.5/0.5. How can I have them hugging each other and still set TOP to a maximal height?\n",
      "Preferable in XML.\n",
      "\n",
      "\n",
      "\n",
      "OVZ6 DEBIAN 9 error: fs.inotify.max_user_watches 8192 how to increase to 524288?\n",
      "\n",
      " The same webhost fixed this fails in other of our OVZ6 servers in London increasing the value. How can we manage this situation without losing all the configuration work? Raising doubts: Are yarn/npm so heavies for this environments? Could be this problem due an old OVZ server kernel?   Our development environment is: Debian GNU/Linux 9 (stretch), Linux 4.9.0-1-amd64, x86-64.  Apache 2.4.25 (Debian).  Mysql 15.1 Distrib 10.1.26-MariaDB, debian-linux-gnu (x86_64), readline 5.2. Phusion Passenger 5.1.12.  Rvm 1.29.3.  Ruby 2.4.3p205 (2017-12-14 revision 61247) [x86_64-linux].  Rails 5.1.4.  Yarn v1.3.2.  Npm 3.10.10.  Webpack 3.10.0 Thanks in advance!\n",
      "\n",
      "\n",
      "\n",
      "Grails generate-all generates a Service class ( 3.2.3 )\n",
      "\n",
      " Since new Grails version 3.2.3 the command generate-all  generates a service grails.gorm.services.Service named Service which is an interface Where can I find documentation about it/ where is the actual implementation I can edit?\n",
      "\n",
      "\n",
      "\n",
      "making .exe using native bundle not work for 32 bit\n",
      "\n",
      " i make an desktop application using javafx, i make an .exe file using native bundle in netbeans 8.1 , this .exe file work proper in 64 bit computer , but in 32 bit computer makes an error like \" MSVCR100.dll is either designed to run on windows or it contains an error \"\n",
      "\n",
      "\n",
      "\n",
      "Move Webpack bootstraper code to it's own file\n",
      "\n",
      " I'm using webpack to bundle my project.  I have several entry points and I'm using the CommonsChunkPlugin.  When webpack creates the bundles it puts it's bootstrapper code in one of the bundles (it's the vendor bundle defined in the CommonsChunkPlugin).  I'm not sure how it decides which bundle to put it in but I'd either like to be able to specify the bundle it's put into or just put it into it's own .js file.  Is this possible? I'm using webpack 1.13.2.  \n",
      "\n",
      "\n",
      "\n",
      "fetch rate for fetching data from wikipedia webservice\n",
      "\n",
      " I need to fetch around 10'000 texts from wikipedia for analysing purpose. Actually I pause the fetching process for 3 seconds between the fetches to not burden the wikipedia service. I fear otherwise my ip will be restricted. Is there any regulation how many wikipedia webapi request can be done in a specific timeslot ? \n",
      "\n",
      "\n",
      "\n",
      "Selenium WebDriver Bot Detection\n",
      "\n",
      " I'm implementing a simple webscraper that scraps data from a big jobs portal. This is the sixth jobs portal that I scrap and only this one is driving me crazy. The others six jobs portal implments bot controls but only this one is blocking me. I have implemented random delays, random javascript scrolls... But after few actions the portal logs me out and ask me for captcha control... Can you provide me some guidlines to follow in order to try to get the content? I just have to scrap few pages ( may be just 100 pages ), but after I get 4-5 pages, it logs me out... Im using Selenium WebDriver in a .Net Application, but I think that this detail is not important... Thanks to support\n",
      "\n",
      "\n",
      "\n",
      "Sidekiq does not release memory after job is processed\n",
      "\n",
      " I have a ruby on rails app, where we validate records from huge excel files(200k records) in background via sidekiq. We also use docker and hence a separate container for sidekiq. When the sidekiq is up, memory used is approx 120Mb, but as the validation worker begins, the memory reaches upto 500Mb (that's after a lot of optimisation).\n",
      "Issue is even after the job is processed, the memory usage stays at 500Mb and is never freed, not allowing any new jobs to be added.\n",
      "I manually start garbage collection using GC.start after every 10k records and also after the job is complete, but still no help.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, text in dataset[['PText', 'Title']][mask].iterrows():\n",
    "    print(text['Title'])\n",
    "    print()\n",
    "    print(text['PText'])\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
